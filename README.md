# covid apiê°œë°œ+pysparkìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬+ airflowë¡œ ìŠ¤ì¼€ì¥´ë§+mysqlë¡œ data insert+csvíŒŒì¼ë¡œ ì €ì¥í•˜ê¸° project

# ë°ì´í„° ETL í”„ë¡œì„¸ìŠ¤

1. ê³µê³µë°ì´í„° apiì—ì„œ xmlí˜•íƒœ(lxml)ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜´(pip install lxml)
2. ë¶ˆëŸ¬ì˜¨ xmlë°ì´í„°ë¥¼ listë¡œ ë§Œë“¤ìŒ
3. listë¥¼ ìŠ¤í‚¤ë§ˆ ì…í˜€ pyspark dataframeìœ¼ë¡œì¨ ìƒì„±
4. ìƒì„±ëœ dataframeì„ ì…ë ¥ë°›ì•„ ì „ì²´ ë°ì´í„°, ì˜¤ëŠ˜ ë°ì´í„°, ì…ë ¥ë°›ì€ ê¸°ê°„ ë™ì•ˆì˜ ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ êµ¬í˜„
5. pyspark dataframeì„ csvíŒŒì¼ë¡œ ì €ì¥, ê¸°ì¤€ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ partitionedëœ csvíŒŒì¼ë¡œ ì €ì¥, mysql DBì— ì…ë ¥ í•˜ê²Œ ë” í•˜ëŠ” í•¨ìˆ˜ êµ¬í˜„(ì „ì²´ ë°ì´í„°, ì˜¤ëŠ˜ ë°ì´í„°, ì…ë ¥ë°›ì€ ê¸°ê°„ ë™ì•ˆì˜ ë°ì´í„° ë³„ë¡œ ì €ì¥ë˜ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •)
6. ì˜¤ëŠ˜ ë°ì´í„°ê°€ ëˆ„ë½ ë˜ì–´ typeì—ëŸ¬ë¥¼ ë°œìƒ ì‹œí‚¨ë‹¤ë©´, daily batchë¥¼ í•˜ì§€ì•Šê³  ì•ˆë‚´ë¬¸ ì¶œë ¥,
7. ì˜¤ëŠ˜ ë°ì´í„°ê°€ ëˆ„ë½ ë˜ì§€ ì•Šì•˜ë‹¤ë©´, daily batch ìˆ˜í–‰(mysqlì— ë°ì´í„° ì €ì¥, íŒŒí‹°ì…˜ë³„ë¡œ ë°ì´í„° ì €ì¥, ì „ì²´ ë°ì´í„° overwriteí•˜ì—¬ ì €ì¥)
8. í´ë¼ìš°ë“œì—ì„œëŠ” s3ë¡œë„ ë³´ë‚´ëŠ” êµ¬ì¡°
    1. s3ë¡œ ë°ì´í„° ë³´ë‚´ê¸° mysql ì—ì„œ ë³´ë‚´ëŠ”ê²ƒ, fileì„ ë³´ë‚´ëŠ” ë²•, s3ë¡œ ì§ì ‘ ì“°ëŠ” ë²•
    2. s3 ì •ì±…ê³¼ public access write permission í™•ì¸í•˜ê¸°
    3. boto3
    

**api_development_xml_pysaprk.py ì‘ì„±(on windows)**

```python
#ìŠ¤íŒŒí¬ê°€ pandasë³´ë‹¤ ëŠë¦¬ê³ , ìŠ¤íŒŒí¬ëŠ” í° ë°ì´í„°ê°€ ì•„ë‹ˆë©´ ì˜ë¯¸ì—†ì§€ë§Œ, ìŠ¤íŒŒí¬ì—ì„œ ë°°ìš´ data transformationì„ í™œìš© í•˜ê¸°ìœ„í•´ ìŠ¤íŒŒí¬ë¥¼ ì±„íƒ
#with coronaê°€ ì‹œí–‰ë¨ì— ë”°ë¼ ê²€ì‚¬ìˆ˜ë¥¼ ë” ì´ìƒ ì¸¡ì •ì„ ì•ˆí•˜ê²Œë¨
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date, timedelta
from typing import Union
import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import Row
from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType, TimestampType, StringType
#sparksession ë“œë¼ì´ë²„ í”„ë¡œì„¸ìŠ¤ ì–»ê¸°
spark = SparkSession.builder.master("local[*]").config("spark.driver.extraClassPath","C:/spark/spark-3.1.2-bin-hadoop2.7/jars/mysql-connector-java-8.0.28").appName("pyspark").getOrCreate()

now = datetime.now()
today = date.today()
oneday = timedelta(days=1)

#RESTapië¡œ ë¶€í„° ë°ì´í„°ë¥¼ ì–»ì–´ì˜¨ë‹¤.
def getCovid19Info(start_date: date, end_date: date):
    url = 'http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19InfStateJson'
    api_key_utf8 = 'mrAZG1yevJBgcaaSuVLOgJ%2BS6blzA0SXlGYZrwxwpARTaMnSotfqFooTr6dgKpPcTBtE96l0xE%2B%2BmXxDrWt19g%3D%3D'
    api_key_decode = requests.utils.unquote(api_key_utf8, encoding='utf-8')

    params ={
        'serviceKey' : api_key_decode,
        'startCreateDt' : int('{:04d}{:02d}{:02d}'.format(start_date.year, start_date.month, start_date.day)),
        'endCreateDt' : int('{:04d}{:02d}{:02d}'.format(end_date.year, end_date.month, end_date.day)),
    }

    response = requests.get(url, params=params)
    content = response.text
    elapsed_us = response.elapsed.microseconds
    print('Reqeust Done, Elapsed: {} seconds'.format(elapsed_us / 1e6))#100ë§Œ
    
    return BeautifulSoup(content, "lxml")#ë¶ˆëŸ¬ì˜¨ text ë°ì´í„°ë¥¼ python xmlì¸ lxmlë¡œ ë³€í™˜

#unionì„ í†µí•´ date, datetime ë‘ê°œì˜ typeëª¨ë‘ í—ˆìš©
def getCovid19SparkDataFrame(start_date: Union[date, datetime], end_date: Union[date,datetime]):
    #keyê°’ì˜ ë°ì´í„° íƒ€ì… ì •ì˜
    convert_method = {
        'accdefrate' : float,
        'accexamcnt' : int,
        'createdt' : lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'),
        'deathcnt' : int,
        'decidecnt' : int,
        'seq' : int,
        'statedt' : str,
        'statetime' : str,
        'updatedt' : lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'),
    }

    temp = getCovid19Info(start_date, end_date)
    items = temp.find('items')#itemë§Œ ì¶”ì¶œ
    item_list = []
    for item in items:
        item_dict = {}
        for tag in list(item):
            try:
                item_dict[tag.name] = convert_method[tag.name](tag.text)
            except Exception:
                item_dict[tag.name] = None
        item_list.append(item_dict)#lxmlë°ì´í„°ë¥¼ dictionary in listë¡œ ë³€í™˜

    #dictionaryì˜ listë¥¼ spark DataFrameìœ¼ë¡œ ë°”ê¾¸ëŠ” ë°©ë²• dictionary ì†ì˜ keyë¥¼ schemaë¥¼ ì •ì˜í•´ì¤€ë‹¤.
    CovidInfo_item_Schema = StructType([
        StructField('accdefrate', DoubleType(), True),#ëˆ„ì í™•ì§„ë¥ 
        StructField('accexamcnt', IntegerType(), True),#ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ì
        StructField('createdt', TimestampType(),True),#ë“±ë¡ì¼ì‹œë¶„ì´ˆ
        StructField('deathcnt', IntegerType(), True),#ì‚¬ë§ì ìˆ˜
        StructField('decidecnt', IntegerType(), True),#í™•ì§„ì ìˆ˜
        StructField('seq', IntegerType(), True),#ê²Œì‹œê¸€ ë²ˆí˜¸(ê°ì—¼í˜„í™© ê³ ìœ ê°’)
        StructField('statedt', StringType(), True),#ê¸°ì¤€ì¼
        StructField('statetime', StringType(), True),#ê¸°ì¤€ì‹œê°„
        StructField('updatedt', TimestampType(), True)#ìˆ˜ì •ì¼ì‹œë¶„ì´ˆ
    ])
    
    df_Covid19 = spark.createDataFrame(item_list,CovidInfo_item_Schema)
    return df_Covid19

#XMLë¡œ ë¶€í„° íŒŒì‹±ëœ itemë“¤ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¦¬í„´
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovid19Data().show()
def getAllCovid19Data():
    df_Covid19 = getCovid19SparkDataFrame(date(2019,1,1), now)
    print("Today: {}\nLoaded {} Records".format(now.strftime('%Y-%m-%d'),df_Covid19.distinct().count()))
    return df_Covid19

#ì½”ë¡œë‚˜ ë§¤ì¼ì˜ í™•ì§„ì, ëˆ„ì í™•ì§„ì, ì‚¬ë§ì, ëˆ„ì ì‚¬ë§ì, ì¹˜ëª…ë¥ , ê²€ì‚¬ì,ëˆ„ì ê²€ì‚¬ìë¥¼ ë½‘ì•„ë‚¸ dataframe
#ì„ì‹œ ì„ ë³„ ê²€ì‚¬ì ìˆ˜ëŠ” ì œì™¸(apië¥¼ ëª»êµ¬í•¨)
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovidAffectedNumbers().show()
def getAllCovidAffectedNumbers():
    from pyspark.sql import Window
    df_Covid19 =  getCovid19SparkDataFrame(date(2019,1,1), now)
    df_Covid19_result_All=\
    df_Covid19.distinct()\
              .withColumn("decidecnt-1",F.coalesce(F.lead(F.col("decidecnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("deathcnt-1",F.coalesce(F.lead(F.col("deathcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("accexamcnt-1",F.coalesce(F.lead(F.col("accexamcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("ê¸°ì¤€ë‚ ì§œ",F.from_unixtime(F.unix_timestamp(F.col("statedt"),"yyyyMMdd"),"yyyy-MM-dd"))\
              .select(F.col("ê¸°ì¤€ë‚ ì§œ")
                     ,(F.col("decidecnt")-F.col("decidecnt-1")).alias("ë‹¹ì¼í™•ì§„ììˆ˜")
                     ,F.col("decidecnt").alias("ëˆ„ì í™•ì§„ììˆ˜")
                     ,(F.col("deathcnt")-F.col("deathcnt-1")).alias("ë‹¹ì¼ì‚¬ë§ììˆ˜")
                     ,F.col("deathcnt").alias("ëˆ„ì ì‚¬ë§ììˆ˜")
                     ,(F.col("accexamcnt")-F.col("accexamcnt-1")).alias("ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.col("accexamcnt").alias("ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.round((F.col("deathcnt")/F.col("decidecnt")*F.lit(100)),2).alias("ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)"))
    return df_Covid19_result_All

#ì½”ë¡œë‚˜ ì˜¤ëŠ˜ì˜ í™•ì§„ì, ëˆ„ì í™•ì§„ì, ì‚¬ë§ì, ëˆ„ì ì‚¬ë§ì, ì¹˜ëª…ë¥ , ê²€ì‚¬ì,ëˆ„ì ê²€ì‚¬ìë¥¼ ë½‘ì•„ë‚¸ dataframe
#ì„ì‹œ ì„ ë³„ ê²€ì‚¬ì ìˆ˜ëŠ” ì œì™¸(apië¥¼ ëª»êµ¬í•¨)
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovidAffectedNumbers().show()    
def getTodayCovidAffectedNumbers():
    from pyspark.sql import Window
    now_date=now.strftime('%Y-%m-%d')
    df_Covid19 =  getCovid19SparkDataFrame(date(2019,1,1), now)
    df_Covid19_result_Today=\
    df_Covid19.distinct()\
              .withColumn("decidecnt-1",F.coalesce(F.lead(F.col("decidecnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("deathcnt-1",F.coalesce(F.lead(F.col("deathcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("accexamcnt-1",F.coalesce(F.lead(F.col("accexamcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("ê¸°ì¤€ë‚ ì§œ",F.from_unixtime(F.unix_timestamp(F.col("statedt"),"yyyyMMdd"),"yyyy-MM-dd"))\
              .where(F.col("ê¸°ì¤€ë‚ ì§œ")==now_date)\
              .select(F.col("ê¸°ì¤€ë‚ ì§œ")
                     ,(F.col("decidecnt")-F.col("decidecnt-1")).alias("ë‹¹ì¼í™•ì§„ììˆ˜")
                     ,F.col("decidecnt").alias("ëˆ„ì í™•ì§„ììˆ˜")
                     ,(F.col("deathcnt")-F.col("deathcnt-1")).alias("ë‹¹ì¼ì‚¬ë§ììˆ˜")
                     ,F.col("deathcnt").alias("ëˆ„ì ì‚¬ë§ììˆ˜")
                     ,(F.col("accexamcnt")-F.col("accexamcnt-1")).alias("ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.col("accexamcnt").alias("ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.round((F.col("deathcnt")/F.col("decidecnt")*F.lit(100)),2).alias("ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)"))
    return df_Covid19_result_Today

#ì½”ë¡œë‚˜ ë²”ìœ„ë‚´ì˜ í™•ì§„ì, ëˆ„ì í™•ì§„ì, ì‚¬ë§ì, ëˆ„ì ì‚¬ë§ì, ì¹˜ëª…ë¥ , ê²€ì‚¬ì,ëˆ„ì ê²€ì‚¬ìë¥¼ ë½‘ì•„ë‚¸ dataframe
#ì„ì‹œ ì„ ë³„ ê²€ì‚¬ì ìˆ˜ëŠ” ì œì™¸(apië¥¼ ëª»êµ¬í•¨)
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovidAffectedNumbers().show()
def getPeriodCovidAffectedNumbers(start_date: Union[date, datetime], end_date: Union[date,datetime]):
    from pyspark.sql import Window
    df_Covid19 =  getCovid19SparkDataFrame(date(2019,1,1), now)
    df_Covid19_result_Period=\
    df_Covid19.distinct()\
              .withColumn("decidecnt-1",F.coalesce(F.lead(F.col("decidecnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("deathcnt-1",F.coalesce(F.lead(F.col("deathcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("accexamcnt-1",F.coalesce(F.lead(F.col("accexamcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("ê¸°ì¤€ë‚ ì§œ",F.from_unixtime(F.unix_timestamp(F.col("statedt"),"yyyyMMdd"),"yyyy-MM-dd"))\
              .where(F.col("ê¸°ì¤€ë‚ ì§œ").between(start_date,end_date))\
              .select(F.col("ê¸°ì¤€ë‚ ì§œ")
                     ,(F.col("decidecnt")-F.col("decidecnt-1")).alias("ë‹¹ì¼í™•ì§„ììˆ˜")
                     ,F.col("decidecnt").alias("ëˆ„ì í™•ì§„ììˆ˜")
                     ,(F.col("deathcnt")-F.col("deathcnt-1")).alias("ë‹¹ì¼ì‚¬ë§ììˆ˜")
                     ,F.col("deathcnt").alias("ëˆ„ì ì‚¬ë§ììˆ˜")
                     ,(F.col("accexamcnt")-F.col("accexamcnt-1")).alias("ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.col("accexamcnt").alias("ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.round((F.col("deathcnt")/F.col("decidecnt")*F.lit(100)),2).alias("ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)"))
    return df_Covid19_result_Period, start_date, end_date#ì´í›„ì— í•¨ìˆ˜ë³„ë¡œ ì €ì¥í•  ë•Œ, DFê°€ ê¸°ê°„ìœ¼ë¡œ ì…ë ¥ë°›ì€ ê²ƒì„ ì•Œê¸°ìœ„í•´ ê¸°ê°„ê°’ë„ ë¦¬í„´

#ìœ„ì˜ 3ê°œì˜ ë©”ì†Œë“œë¥¼ í†µí•´ ë°ì´í„°í”„ë ˆì„ì„ ì…ë ¥ë°›ì•„ cë“œë¼ì´ë¸Œ covid19_result í´ë”ì— í•¨ìˆ˜ í˜•íƒœì— ë”°ë¥¸csvíŒŒì¼ë¡œ ì €ì¥

def saveDataAsCSV(dataframe):
    if type(dataframe) == tuple:
        dataframe[0].cache()
        start_date=dataframe[1]
        end_date=dataframe[2]
        if dataframe[0].count() == getPeriodCovidAffectedNumbers(start_date, end_date)[0].count():
            print("it worked!")
            return dataframe[0].coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("C:/covid19_result/period/")
    else:
        dataframe.cache()
        if dataframe.count() == getAllCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("C:/covid19_result/all_day/")
        elif dataframe.count() == getTodayCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("C:/covid19_result/today/")

#íŒŒí‹°ì…˜ë³„ë¡œ ë‚˜ëˆ„ì–´ íŒŒì¼ ì €ì¥(daily partitionë³„ë¡œ ë°ì´í„° ì ì¬)        
def saveDataAsPartitionCSV(dataframe):
    if type(dataframe) == tuple:
        dataframe[0].cache()
        start_date=dataframe[1]
        end_date=dataframe[2]
        if dataframe[0].count() == getPeriodCovidAffectedNumbers(start_date, end_date)[0].count():
            print("it worked!")
            return dataframe[0].write.format("csv").option("header","true").mode("overwrite").partitionBy("ê¸°ì¤€ë‚ ì§œ").save("C:/covid19_result/partition/period/")
    else:
        dataframe.cache()
        if dataframe.count() == getAllCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.write.format("csv").option("header","true").mode("overwrite").partitionBy("ê¸°ì¤€ë‚ ì§œ").save("C:/covid19_result/partition/all_day/")
        elif dataframe.count() == getTodayCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.write.format("csv").option("header","true").mode("overwrite").partitionBy("ê¸°ì¤€ë‚ ì§œ").save("C:/covid19_result/partition/today/")

#mysql DBì— Covid19 DataFrameì˜ ë°ì´í„°ë¥¼ ì €ì¥
def saveDataToMySQL(dataframe):
    if type(dataframe) == tuple:
        dataframe[0].cache()
    else:
        dataframe.cache()
        
    if dataframe.count() == getAllCovidAffectedNumbers().count():
        print("insert to mysql Covid19 table")
        return dataframe.coalesce(1).write.format("jdbc").options(
            url='jdbc:mysql://localhost:3306/COVID19',
            driver='com.mysql.cj.jdbc.Driver',
            dbtable='Covid_19_info',
            user='root',
            password='root'
        ).mode('overwrite').save()
    
if __name__=="__main__":
    try:
        today_infection_num = (getTodayCovidAffectedNumbers().first()["ë‹¹ì¼í™•ì§„ììˆ˜"])
        infection_num_diff = (getTodayCovidAffectedNumbers().first()["ë‹¹ì¼í™•ì§„ììˆ˜"] - 
                                                     getPeriodCovidAffectedNumbers(today-oneday,today)[0].where(F.col("ê¸°ì¤€ë‚ ì§œ")==str(today-oneday)).first()["ë‹¹ì¼í™•ì§„ììˆ˜"])
        print("ì˜¤ëŠ˜(%s)ì˜ í™•ì§„ììˆ˜ëŠ” %dëª…ì…ë‹ˆë‹¤.\n" % (today, today_infection_num))#df.first()['column name'] í˜¹ì€ df.collect()[0]['column name'], ì˜¤ëŠ˜ì˜ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° none typeì´ ë˜ì–´ ì—ëŸ¬ë¥¼ ë‚¸ë‹¤.try except ì²˜ë¦¬
        if infection_num_diff >= 0:
            print("ì–´ì œë³´ë‹¤ ì½”ë¡œë‚˜ í™•ì§„ìê°€ %dëª… ëŠ˜ì—ˆìŠµë‹ˆë‹¤.\n" % (infection_num_diff))
        else:
            print("ì–´ì œë³´ë‹¤ ì½”ë¡œë‚˜ í™•ì§„ìê°€ %dëª… ì¤„ì—ˆìŠµë‹ˆë‹¤.\n" % (-infection_num_diff))
    except TypeError:
        print("ì˜¤ëŠ˜ì˜ ë°ì´í„°ê°€ ì•„ì§ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    saveDataAsCSV(getAllCovidAffectedNumbers())
    saveDataAsPartitionCSV(getAllCovidAffectedNumbers())
    saveDataToMySQL(getAllCovidAffectedNumbers())
```

**api_development_xml_pysaprk.py ì‘ì„±(on ec2)**

```python
#ìŠ¤íŒŒí¬ê°€ pandasë³´ë‹¤ ëŠë¦¬ê³ , ìŠ¤íŒŒí¬ëŠ” í° ë°ì´í„°ê°€ ì•„ë‹ˆë©´ ì˜ë¯¸ì—†ì§€ë§Œ, ìŠ¤íŒŒí¬ì—ì„œ ë°°ìš´ data transformationì„ í™œìš© í•˜ê¸°ìœ„í•´ ìŠ¤íŒŒí¬ë¥¼ ì±„íƒ
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date, timedelta
from typing import Union
import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import Row
from pyspark.sql.types import StructField, StructType, DoubleType, IntegerType, TimestampType, StringType
#sparksession ë“œë¼ì´ë²„ í”„ë¡œì„¸ìŠ¤ ì–»ê¸°
spark = SparkSession.builder.master("local[*]").config("spark.driver.extraClassPath","/home/ubuntu/mysql-connector-java-8.0.28/mysql-connector-java-8.0.28.jar").appName("pyspark").getOrCreate()

now = datetime.now()
today = date.today()
oneday = timedelta(days=1)

def getCovid19Info(start_date: date, end_date: date):
    url = 'http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19InfStateJson'
    api_key_utf8 = 'mrAZG1yevJBgcaaSuVLOgJ%2BS6blzA0SXlGYZrwxwpARTaMnSotfqFooTr6dgKpPcTBtE96l0xE%2B%2BmXxDrWt19g%3D%3D'
    api_key_decode = requests.utils.unquote(api_key_utf8, encoding='utf-8')

    params ={
        'serviceKey' : api_key_decode,
        'startCreateDt' : int('{:04d}{:02d}{:02d}'.format(start_date.year, start_date.month, start_date.day)),
        'endCreateDt' : int('{:04d}{:02d}{:02d}'.format(end_date.year, end_date.month, end_date.day)),
    }

    response = requests.get(url, params=params)
    content = response.text
    elapsed_us = response.elapsed.microseconds
    print('Reqeust Done, Elapsed: {} seconds'.format(elapsed_us / 1e6))#100ë§Œ
    
    return BeautifulSoup(content, "lxml")#python xmlì¸ lxmlë¡œ ë³€í™˜

#unionì„ í†µí•´ date, datetime ë‘ê°œì˜ typeëª¨ë‘ í—ˆìš©
def getCovid19SparkDataFrame(start_date: Union[date, datetime], end_date: Union[date,datetime]):
    #keyê°’ì˜ ë°ì´í„° íƒ€ì… ì •ì˜
    convert_method = {
        'accdefrate' : float,
        'accexamcnt' : int,
        'createdt' : lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'),
        'deathcnt' : int,
        'decidecnt' : int,
        'seq' : int,
        'statedt' : str,
        'statetime' : str,
        'updatedt' : lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'),
    }
    #íŒŒì‹± í›„ dictionayë“¤ì˜ list í˜•íƒœ ë³€í™˜ ê³¼ì • ì´í•´í•˜ê¸°
    temp = getCovid19Info(start_date, end_date)
    items = temp.find('items')
    item_list = []
    for item in items:
        item_dict = {}
        for tag in list(item):
            try:
                item_dict[tag.name] = convert_method[tag.name](tag.text)
            except Exception:
                item_dict[tag.name] = None
        item_list.append(item_dict)

    #dictionaryì˜ listë¥¼ spark DataFrameìœ¼ë¡œ ë°”ê¾¸ëŠ” ë°©ë²• dictionary ì†ì˜ keyë¥¼ schemaë¥¼ ì •ì˜í•´ì¤€ë‹¤.
    CovidInfo_item_Schema = StructType([
        StructField('accdefrate', DoubleType(), True),#ëˆ„ì í™•ì§„ë¥ 
        StructField('accexamcnt', IntegerType(), True),#ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ì
        StructField('createdt', TimestampType(),True),#ë“±ë¡ì¼ì‹œë¶„ì´ˆ
        StructField('deathcnt', IntegerType(), True),#ì‚¬ë§ì ìˆ˜
        StructField('decidecnt', IntegerType(), True),#í™•ì§„ì ìˆ˜
        StructField('seq', IntegerType(), True),#ê²Œì‹œê¸€ ë²ˆí˜¸(ê°ì—¼í˜„í™© ê³ ìœ ê°’)
        StructField('statedt', StringType(), True),#ê¸°ì¤€ì¼
        StructField('statetime', StringType(), True),#ê¸°ì¤€ì‹œê°„
        StructField('updatedt', TimestampType(), True)#ìˆ˜ì •ì¼ì‹œë¶„ì´ˆ
    ])
    
    df_Covid19 = spark.createDataFrame(item_list,CovidInfo_item_Schema)
    return df_Covid19

#XMLë¡œ ë¶€í„° íŒŒì‹±ëœ itemë“¤ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¦¬í„´
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovid19Data().show()
def getAllCovid19Data():
    df_Covid19 = getCovid19SparkDataFrame(date(2019,1,1), now)
    print("Today: {}\nLoaded {} Records".format(now.strftime('%Y-%m-%d'),df_Covid19.distinct().count()))
    return df_Covid19

#ì½”ë¡œë‚˜ ë§¤ì¼ì˜ í™•ì§„ì, ëˆ„ì í™•ì§„ì, ì‚¬ë§ì, ëˆ„ì ì‚¬ë§ì, ì¹˜ëª…ë¥ , ê²€ì‚¬ì,ëˆ„ì ê²€ì‚¬ìë¥¼ ë½‘ì•„ë‚¸ dataframe
#ì„ì‹œ ì„ ë³„ ê²€ì‚¬ì ìˆ˜ëŠ” ì œì™¸(apië¥¼ ëª»êµ¬í•¨)
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovidAffectedNumbers().show()
def getAllCovidAffectedNumbers():
    from pyspark.sql import Window
    df_Covid19 =  getCovid19SparkDataFrame(date(2019,1,1), now)
    df_Covid19_result_All=\
    df_Covid19.distinct()\
              .withColumn("decidecnt-1",F.coalesce(F.lead(F.col("decidecnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("deathcnt-1",F.coalesce(F.lead(F.col("deathcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("accexamcnt-1",F.coalesce(F.lead(F.col("accexamcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("ê¸°ì¤€ë‚ ì§œ",F.from_unixtime(F.unix_timestamp(F.col("statedt"),"yyyyMMdd"),"yyyy-MM-dd"))\
              .select(F.col("ê¸°ì¤€ë‚ ì§œ")
                     ,(F.col("decidecnt")-F.col("decidecnt-1")).alias("ë‹¹ì¼í™•ì§„ììˆ˜")
                     ,F.col("decidecnt").alias("ëˆ„ì í™•ì§„ììˆ˜")
                     ,(F.col("deathcnt")-F.col("deathcnt-1")).alias("ë‹¹ì¼ì‚¬ë§ììˆ˜")
                     ,F.col("deathcnt").alias("ëˆ„ì ì‚¬ë§ììˆ˜")
                     ,(F.col("accexamcnt")-F.col("accexamcnt-1")).alias("ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.col("accexamcnt").alias("ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.round((F.col("deathcnt")/F.col("decidecnt")*F.lit(100)),2).alias("ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)"))
    return df_Covid19_result_All

#ì½”ë¡œë‚˜ ì˜¤ëŠ˜ì˜ í™•ì§„ì, ëˆ„ì í™•ì§„ì, ì‚¬ë§ì, ëˆ„ì ì‚¬ë§ì, ì¹˜ëª…ë¥ , ê²€ì‚¬ì,ëˆ„ì ê²€ì‚¬ìë¥¼ ë½‘ì•„ë‚¸ dataframe
#ì„ì‹œ ì„ ë³„ ê²€ì‚¬ì ìˆ˜ëŠ” ì œì™¸(apië¥¼ ëª»êµ¬í•¨)
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovidAffectedNumbers().show()    
def getTodayCovidAffectedNumbers():
    from pyspark.sql import Window
    now_date=now.strftime('%Y-%m-%d')
    df_Covid19 =  getCovid19SparkDataFrame(date(2019,1,1), now)
    df_Covid19_result_Today=\
    df_Covid19.distinct()\
              .withColumn("decidecnt-1",F.coalesce(F.lead(F.col("decidecnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("deathcnt-1",F.coalesce(F.lead(F.col("deathcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("accexamcnt-1",F.coalesce(F.lead(F.col("accexamcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("ê¸°ì¤€ë‚ ì§œ",F.from_unixtime(F.unix_timestamp(F.col("statedt"),"yyyyMMdd"),"yyyy-MM-dd"))\
              .where(F.col("ê¸°ì¤€ë‚ ì§œ")==now_date)\
              .select(F.col("ê¸°ì¤€ë‚ ì§œ")
                     ,(F.col("decidecnt")-F.col("decidecnt-1")).alias("ë‹¹ì¼í™•ì§„ììˆ˜")
                     ,F.col("decidecnt").alias("ëˆ„ì í™•ì§„ììˆ˜")
                     ,(F.col("deathcnt")-F.col("deathcnt-1")).alias("ë‹¹ì¼ì‚¬ë§ììˆ˜")
                     ,F.col("deathcnt").alias("ëˆ„ì ì‚¬ë§ììˆ˜")
                     ,(F.col("accexamcnt")-F.col("accexamcnt-1")).alias("ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.col("accexamcnt").alias("ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.round((F.col("deathcnt")/F.col("decidecnt")*F.lit(100)),2).alias("ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)"))
    return df_Covid19_result_Today

#ì½”ë¡œë‚˜ ë²”ìœ„ë‚´ì˜ í™•ì§„ì, ëˆ„ì í™•ì§„ì, ì‚¬ë§ì, ëˆ„ì ì‚¬ë§ì, ì¹˜ëª…ë¥ , ê²€ì‚¬ì,ëˆ„ì ê²€ì‚¬ìë¥¼ ë½‘ì•„ë‚¸ dataframe
#ì„ì‹œ ì„ ë³„ ê²€ì‚¬ì ìˆ˜ëŠ” ì œì™¸(apië¥¼ ëª»êµ¬í•¨)
#ë°ì´í„° ì¡°íšŒì‹œ ë©”ì†Œë“œ.show() ex)getAllCovidAffectedNumbers().show()
def getPeriodCovidAffectedNumbers(start_date: Union[date, datetime], end_date: Union[date,datetime]):
    from pyspark.sql import Window
    df_Covid19 =  getCovid19SparkDataFrame(date(2019,1,1), now)
    df_Covid19_result_Period=\
    df_Covid19.distinct()\
              .withColumn("decidecnt-1",F.coalesce(F.lead(F.col("decidecnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("deathcnt-1",F.coalesce(F.lead(F.col("deathcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("accexamcnt-1",F.coalesce(F.lead(F.col("accexamcnt"),1).over(Window.orderBy(F.col("statedt").desc())),F.lit(0)))\
              .withColumn("ê¸°ì¤€ë‚ ì§œ",F.from_unixtime(F.unix_timestamp(F.col("statedt"),"yyyyMMdd"),"yyyy-MM-dd"))\
              .where(F.col("ê¸°ì¤€ë‚ ì§œ").between(start_date,end_date))\
              .select(F.col("ê¸°ì¤€ë‚ ì§œ")
                     ,(F.col("decidecnt")-F.col("decidecnt-1")).alias("ë‹¹ì¼í™•ì§„ììˆ˜")
                     ,F.col("decidecnt").alias("ëˆ„ì í™•ì§„ììˆ˜")
                     ,(F.col("deathcnt")-F.col("deathcnt-1")).alias("ë‹¹ì¼ì‚¬ë§ììˆ˜")
                     ,F.col("deathcnt").alias("ëˆ„ì ì‚¬ë§ììˆ˜")
                     ,(F.col("accexamcnt")-F.col("accexamcnt-1")).alias("ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.col("accexamcnt").alias("ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜")
                     ,F.round((F.col("deathcnt")/F.col("decidecnt")*F.lit(100)),2).alias("ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)"))
    return df_Covid19_result_Period, start_date, end_date

#ìœ„ì˜ 3ê°œì˜ ë©”ì†Œë“œë¥¼ í†µí•´ ë°ì´í„°í”„ë ˆì„ì„ ì…ë ¥ë°›ì•„ cë“œë¼ì´ë¸Œ covid19_result í´ë”ì— í•¨ìˆ˜ í˜•íƒœì— ë”°ë¥¸csvíŒŒì¼ë¡œ ì €ì¥
#í•¨ìˆ˜ ë‚´ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¸ì‹ ì‹œí‚¤ëŠ” ë²• ì°¾ê¸°, classë¥¼ í†µí•´ êµ¬í˜„, method function êµ¬ë¶„í•´ì„œ ì™„ì„±ì‹œí‚¤ê¸°
# count() takes exactly one argument (0 given) í•´ê²°í•˜ê¸°
def saveDataAsCSV(dataframe):
    if type(dataframe) == tuple:
        dataframe[0].cache()
        start_date=dataframe[1]
        end_date=dataframe[2]
        if dataframe[0].count() == getPeriodCovidAffectedNumbers(start_date, end_date)[0].count():
            print("it worked!")
            return dataframe[0].coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/home/ubuntu/covid19_result/period/")
    else:
        dataframe.cache()
        if dataframe.count() == getAllCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/home/ubuntu/covid19_result/all_day/")
        elif dataframe.count() == getTodayCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.coalesce(1).write.format("csv").option("header","true").mode("overwrite").save("/home/ubuntu/covid19_result/today/")
        
def saveDataAsPartitionCSV(dataframe):
    if type(dataframe) == tuple:
        dataframe[0].cache()
        start_date=dataframe[1]
        end_date=dataframe[2]
        if dataframe[0].count() == getPeriodCovidAffectedNumbers(start_date, end_date)[0].count():
            print("it worked!")
            return dataframe[0].write.format("csv").option("header","true").mode("overwrite").partitionBy("ê¸°ì¤€ë‚ ì§œ").save("/home/ubuntu/covid19_result/partition/period/")
    else:
        dataframe.cache()
        if dataframe.count() == getAllCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.write.format("csv").option("header","true").mode("overwrite").partitionBy("ê¸°ì¤€ë‚ ì§œ").save("/home/ubuntu/covid19_result/partition/all_day/")
        elif dataframe.count() == getTodayCovidAffectedNumbers().count():
            print("it worked!")
            return dataframe.write.format("csv").option("header","true").mode("append").partitionBy("ê¸°ì¤€ë‚ ì§œ").save("/home/ubuntu/covid19_result/partition/today/")

def saveDataToMySQL(dataframe):
    if type(dataframe) == tuple:
        dataframe[0].cache()
    else:
        dataframe.cache()
        
    if dataframe.count() == getAllCovidAffectedNumbers().count():
        print("insert to mysql Covid19 table")
        return dataframe.coalesce(1).write.format("jdbc").options(
            url='jdbc:mysql://localhost:3306/COVID19',
            driver='com.mysql.cj.jdbc.Driver',
            dbtable='Covid_19_info',
            user='root',
            password='root'
        ).mode('overwrite').save()
    
if __name__=="__main__":
    try:
        today_infection_num = (getTodayCovidAffectedNumbers().first()["ë‹¹ì¼í™•ì§„ììˆ˜"])
        infection_num_diff = (getTodayCovidAffectedNumbers().first()["ë‹¹ì¼í™•ì§„ììˆ˜"] - 
                                                     getPeriodCovidAffectedNumbers(today-oneday,today)[0].where(F.col("ê¸°ì¤€ë‚ ì§œ")==str(today-oneday)).first()["ë‹¹ì¼í™•ì§„ììˆ˜"])
        print("ì˜¤ëŠ˜(%s)ì˜ í™•ì§„ììˆ˜ëŠ” %dëª…ì…ë‹ˆë‹¤.\n" % (today, today_infection_num))#df.first()['column name'] í˜¹ì€ df.collect()[0]['column name'], ì˜¤ëŠ˜ì˜ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° none typeì´ ë˜ì–´ ì—ëŸ¬ë¥¼ ë‚¸ë‹¤.try except ì²˜ë¦¬
        if infection_num_diff >= 0:
            print("ì–´ì œë³´ë‹¤ ì½”ë¡œë‚˜ í™•ì§„ìê°€ %dëª… ëŠ˜ì—ˆìŠµë‹ˆë‹¤.\n" % (infection_num_diff))
        else:
            print("ì–´ì œë³´ë‹¤ ì½”ë¡œë‚˜ í™•ì§„ìê°€ %dëª… ì¤„ì—ˆìŠµë‹ˆë‹¤.\n" % (-infection_num_diff))
    except TypeError:
        print("ì˜¤ëŠ˜ì˜ ë°ì´í„°ê°€ ì•„ì§ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    saveDataAsCSV(getAllCovidAffectedNumbers())
    saveDataAsPartitionCSV(getAllCovidAffectedNumbers())
    saveDataToMySQL(getAllCovidAffectedNumbers())
```

**airflow DAG on EC2**

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'spidyweb',
    'retries': 0,
    'retry_delay': timedelta(seconds=20),
    'depends_on_past': False
}

dag_Spark_api = DAG(
    'dag_Spark_api_id',
    start_date=days_ago(2),
    default_args=default_args,
    schedule_interval='0 1 * * *',
    catchup=False,
    is_paused_upon_creation=False,
)

cmd="python3 /home/ubuntu/api_development_xml_pyspark.py"
cmd2="echo 'copying api_result files to S3' && aws s3 sync /home/ubuntu/covid19_result/partition/all_day/ s3://api-pyspark-airflow-1/api_results_partitioned/ && aws s3 sync /home/ubuntu/covid19_result/all_day/ s3://api-pyspark-airflow-1/api_results_onefile/"
cmd3="echo 'copying airflow log files to S3' && aws s3 sync /home/ubuntu/airflow/logs/dag_Spark_api_batch/spark_api_xml/ s3://api-pyspark-airflow-1/api_airflow_logs/"
cmd4="echo â€˜this server will be terminated after 7minutesâ€™ && sleep 7m && aws ec2 terminate-instances  --instance-id $(ec2metadata --instance-id)"

#ì‹œì‘ì„ ì•Œë¦¬ëŠ” dummy
task_start = DummyOperator(
    task_id='start',
    dag=dag_Spark_api,
)

#ì‹œì‘ì´ ëë‚˜ê³  ë‹¤ìŒë‹¨ê³„ë¡œ ì§„í–‰ë˜ì—ˆìŒì„ ë‚˜íƒ€ë‚´ëŠ” dummy
task_next = DummyOperator(
    task_id='next',
    trigger_rule='all_success',
    dag=dag_Spark_api,
)
#ëì„ ì•Œë¦¬ëŠ” dummy
task_finish = DummyOperator(
    task_id='finish',
    trigger_rule='all_success',
    dag=dag_Spark_api,
)

#ì˜¤ëŠ˜ì˜ ëª©í‘œì¸ bashë¥¼ í†µí•´ python file ì‹¤í–‰ì‹œí‚¬ BashOperator
api_PySpark_1 = BashOperator(
    task_id='spark_api_xml',
    dag=dag_Spark_api,
    trigger_rule='all_success',
    bash_command=cmd,
)

#ìƒì„±ëœ spark ê²°ê³¼ë¬¼ íŒŒì¼ì„ s3ë¡œ ë³µì‚¬í•˜ëŠ” BashOperator
task_Copy_results_to_S3 = BashOperator(
    task_id='copy_results',
    dag=dag_Spark_api,
    trigger_rule='all_success',
    bash_command=cmd2,
)

#airflow log files ë¥¼ s3ë¡œ ë³´ë‚´ëŠ” BashOperator
task_Copy_Log_to_S3 = BashOperator(
    task_id='copy_log',
    dag=dag_Spark_api,
    trigger_rule='all_success',
    bash_command=cmd3,
)

#ec2ë¥¼ ì¢…ë£Œì‹œí‚¤ëŠ” BashOperator
task_terminate_server = BashOperator(
    task_id='terminate_server',
    dag=dag_Spark_api,
    trigger_rule='all_success',
    bash_command=cmd4,
)


#ì˜ì¡´ê´€ê³„ êµ¬ì„±
task_start >> task_next >> api_PySpark_1 >> task_Copy_results_to_S3 >>  task_Copy_Log_to_S3 >> task_finish >> task_terminate_server
```

---

# **ubuntu í™˜ê²½(cloud í™˜ê²½)**

**ë¦¬ëˆ…ìŠ¤ ì‹œìŠ¤í…œ ì‹œê°„ now() ê³ ì¹˜ê¸°** 

```bash
sudo apt-get install rdate
sudo rdate -s time.bora.net
```

**com.cj.mysql connector ë‹¤ìš´ ë°›ê³ , mysqlì— ì €ì¥ë  ë•Œ ê²½ë¡œ ì„¤ì •**

```bash
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.28.zip
sudo apt install unzip
unzip mysql-connector-java-8.0.28.zip
```

**bs4, lxml,findspark ì„¤ì¹˜**

```bash
pip install bs4, lxml, findspark
```

**ec2 security group**

8080í¬íŠ¸ ì—´ì–´ì„œ airflowí™•ì¸ í•  ìˆ˜ìˆê²Œ securitecho â€˜this server will be terminated after 5minutesâ€™ && sleep 5m && aws ec2 terminate-instances  --instance-id $EC2ID

airflow ë¥¼ í†µí•´ ì œì¶œì‹œ $EC2IDê°€ ì¸ì‹ì´ ì•ˆëœë‹¤?

echo ë¡œ ì¸ì‹í•´ë³´ê¸°

ì•ˆë  ì‹œ (ec2metadata --instance-id)ë¡œ ì‚¬ìš©í•´ë³´ê¸° ì—ëŸ¬ë‚œë‹¤

 group tcp 8080í¬íŠ¸ ì—´ê¸°

**EC2 ìë™ ë°°í¬ ë° ìë™ ë„ê¸°**

1. EC2 imageìƒì„±í•˜ì—¬ EC2 image builder, image pipelineì„ ì´ìš©í•˜ì—¬ dailyí•˜ê²Œ ë„ì›€
2. aws clië¥¼ ec2ì— ì„¤ì¹˜
3. ec2 instance idë¥¼ EC2ID ë³€ìˆ˜ë¡œì¨ .profileì— ì €ì¥ (airflowì—ë„ ë³€ìˆ˜ ë“±ë¡?)
    1. EC2ID=$(ec2metadata --instance-id)
    2. /home/ubuntu/.local/bin/airflow variables set EC2ID $EC2ID ë¡œ airflowì— ì „ì—­ ë³€ìˆ˜ê°’ì„ ë“±ë¡ ê·¸ëŸ¼ ë§¤ë²ˆ ì¼œì§ˆ ë•Œë§ˆë‹¤ ë“±ë¡ì€ ì–´ë–»ê²Œ í•˜ì§€? â†’ $(ec2metadata  --instance-id)ì²˜ëŸ¼ ì¤„ê¹Œâ†’ì¼ë‹¨ ì´ ë°©ë²•ì´ ì„±ê³µ
4. airflowë¥¼ í†µí•œ ETL jobì´ ë‹¤ ëë‚˜ê³  5ë¶„ ë’¤ì— aws clië¥¼ í†µí•´ ec2 terminate job ì‹¤í–‰(AWS configureë¡œ access KEY secret ACCESS KEY, region ì…ë ¥ë˜ì–´ì•¼ í•¨
    1. echo â€˜this server will be terminated after 5minutesâ€™ && sleep 5m && aws ec2 terminate-instances  --instance-id $EC2ID
    
    <aside>
    ğŸ“Ÿ spot instanceë¡œ ë„ìš°ê¸°(ì—†ëŠ” ê±° ê°™ë‹¤ ã… ã… )3
    
    </aside>
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/89bb8e54-1716-493f-b3c7-d1d0b51cf08e/Untitled.png)
    
    `Task received SIGTERM signal` ì˜¤ë¥˜ê°€ ëœ¬ë‹¤ â†’ì–´ë–»ê²Œ í•´ê²°? trigger_ruleì„ all_successë¡œ ë³€ê²½?
    
    ---
    

**MySQL ì„¤ì¹˜**

```bash
sudo apt-get update
sudo apt-get install mysql-server
sudoÂ systemctlÂ startÂ mysql
```

**MYSQL ì‹œì‘í•  ë•Œ ìë™ ì‹¤í–‰**

```bash
sudoÂ systemctlÂ enableÂ mysql
```

**DB tableìƒì„±**

```sql
create database COVID19;

use COVID19;

create table Covid_19_info(
`ê¸°ì¤€ë‚ ì§œ` date,
`ë‹¹ì¼í™•ì§„ììˆ˜` int,
`ëˆ„ì í™•ì§„ììˆ˜` int,
`ë‹¹ì¼ì‚¬ë§ììˆ˜` int,
`ëˆ„ì ì‚¬ë§ììˆ˜` int,
`ë‹¹ì¼ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜` int,
`ëˆ„ì ì˜ì‹¬ì‹ ê³ ê²€ì‚¬ììˆ˜` int,
`ì „ì²´í™•ì§„ìì¹˜ëª…ë¥ (%)` float,
PRIMARY KEY(ê¸°ì¤€ë‚ ì§œ)
);
```

**sudo ì—†ì´ ë¡œê·¸ì¸**

```sql
ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'ë¹„ë°€ë²ˆí˜¸';

FLUSH PRIVILEGES;

GRANT ALL PRIVILEGES ON COVID19.* TO 'root'@'localhost';
```

---

**spark ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •**

ë‚´ tistoryblog ì°¸ì¡°

---

# **airflow ì„¤ì •**

**ec2 ì•ˆì—ì„œ ë§¤ì¼ ë°°ì¹˜ì²˜ëŸ¼ ëŒë ¤ë©´ ì¼œì¡Œë‹¤ êº¼ì§€ëŠ” í™˜ê²½ì—ì„œ schedule intervalì€ ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼ë˜ëŠ”ê°€?**

1. daily
2. @once onceí•˜ë©´ ì•ˆëˆë‹¤.

sudo apt-get update

s**udo apt install python3-pip**

pip3 install apache-airflow

export AIRFLOW_HOME=/home/ubuntu/airflow

./airflow db init

cd .local/bin

mkdir dags

./airflow usersÂ createÂ \

--username admin \

--firstname Admin \

--lastname spidyweb \

--role Admin \

--email admin@spidyweb.com

systemdì— ë“±ë¡

ì•„ì§ ì‹œì‘ x

dagsì— íŒŒì¼ ì‘ì„±

aws cliê¸°ë°˜ìœ¼ë¡œ ë³€ìˆ˜ì„¤ì • í•˜ê³ ,aws configureì´í›„ì— ë„ëŠ” ëª…ë ¹ì–´ ì‹¤í–‰

airflowë¡œ shell script í†µí•´ aws ec2 terminate-instances --instance-id $EC2ID ë¨¹íˆëŠ”ì§€ í™•ì¸í•˜ê¸°(2022-02-10) 

**systemd ì— airflow-webserver.service airflow-scheduler.service ë“±ë¡í•˜ê¸°**

dbë¥¼ ì•„ì˜ˆ mysqlë¡œ ë°”ê¿”ì„œ ì§„í–‰í•´ë³´ê¸°

ì˜ ì•ˆë˜ëŠ” ë¡œê·¸ ë³´ê¸°

journalctl _PID=â€™í”„ë¡œì„¸ìŠ¤ì•„ì´ë””â€™

ì„¤ì •ìì²´ëŠ” 2ê°œë‹¤ ë§ëŠ”ê±°ê³  ë„ì›Œì§€ê¸°ë„í•˜ëŠ”ë° ìŠ¤ì¼€ì¥´ëœdagê°€ dbë¥¼ ì°¸ì¡° ëª»í•˜ëŠ” ëŠë‚Œ?

```bash
[Unit]
Description=Airflow webserver
After=network.target mysql.service
wants=mysql.service

[Service]
Environment="PATH=/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
User=ubuntu
Group=ubuntu
Type=simple
ExecStart=/home/ubuntu/.local/bin/airflow webserver -p 8080
Restart=on-failure
RestartSec=10s

[Install]
WantedBy=multi-user.target
```

```bash
[Unit]
Description=Airflow webserver
After=network.target mysql.service
wants=mysql.service

[Service]
Environment="PATH=/home/ubuntu/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
User=ubuntu
Group=ubuntu
Type=simple
ExecStart=/home/ubuntu/.local/bin/airflow scheduler
Restart=on-failure
RestartSec=10s

[Install]
WantedBy=multi-user.target
```

---
**S3ë¡œ íŒŒì¼ì„ ë³µì‚¬í•˜ê¸° ë‹¨ì¼ íŒŒì¼, partitionëœ íŒŒì¼ ë³µì‚¬**

- **ë””ë ‰í† ë¦¬ ë³µì‚¬**

aws s3 sync /home/ubuntu/covid19_result/partition/all_day/ s3://api-pyspark-airflow-1/api_results_partitioned/

- **íŒŒì¼ ë³µì‚¬**

aws s3 sync /home/ubuntu/covid19_result/all_day/ s3://api-pyspark-airflow-1/api_results_onefile/

- **ë¡œê·¸ íŒŒì¼ ë³µì‚¬**

aws s3 sync /home/ubuntu/airflow/logs/dag_Spark_api_id/spark_api_xml/ s3://api-pyspark-airflow-1/api_airflow_logs/

---

**í…ŒìŠ¤íŠ¸í™˜ê²½ ìˆœì„œ(ì™„ì„±)**

ë¡œì»¬ jupyterâ†’ local vm ubuntu â†’ amazon ec2 ubuntu

---

**ubuntu terminalì—ì„œ ìˆ˜í–‰ ê°€ëŠ¥ í•˜ê²Œ ë§Œë“¤ê¸°(ì™„ì„±)**

í„°ë¯¸ë„ì—ì„œ name == mainìœ¼ë¡œ í„°ë¯¸ë„ì—ì„œ ìˆ˜í–‰ ê°€ëŠ¥í•˜ê²Œ ë” êµ¬í˜„

---

**ë°ì´í„° ìˆ˜ì§‘, ì²˜ë¦¬ ì„±ëŠ¥(ì™„ì„±)**

ë°ì´í„° ì—¬ëŸ¬ë²ˆ scaní•˜ëŠ” ê³¼ì • ì¤„ì´ê¸°

pyspark cached í™œìš© í˜¹ì€apië°ì´í„° ë°›ì€ ê²ƒì„ ë³€ìˆ˜ë¡œì¨ ì €ì¥í•˜ì—¬ í•œë²ˆë§Œ í˜¸ì¶œ í•˜ê²Œ ë” í•˜ê¸°

76.6ì´ˆ cached 8core

87ì´ˆ non cahced 8core

---

**cache()**

- cacheëŠ” í•¨ìˆ˜ í˜¸ì¶œë¶€í„° ì‹¤í–‰ ëë‚  ë•Œ ê¹Œì§€ë§Œ ìœ ì§€ë˜ëŠ”ê°€? â†’  o? ë§ëŠ”ì§€ ë‚˜ì¤‘ì— ìˆ˜í–‰
- cacheëŠ” ì¸ìë¡œ ë°›ì€ dataframeìœ¼ë¡œ cacheí•˜ë©´ í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœë¡œ dataframeì„ ë¶ˆëŸ¬ì™€ë„ ê°™ì€ ê²ƒìœ¼ë¡œ ì¸ì‹í•˜ì§€ ì•Šì•„ cacheì˜ íš¨ê³¼ë¥¼ ë³´ì§€ ëª»í•˜ëŠ”ê°€?â†’ == ì²˜ëŸ¼ booleanìœ¼ë¡œ í‘œí˜„í•´ë³¼ ì‹œ falseë¡œ ë‚˜ì˜¨ë‹¤.
- cache()ë¥¼ í•´ë„ count()ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤? â†’ x cacheëŠ” lazy executionì´ë¼ countë•Œ ì²« í˜¸ì¶œë˜ì–´ì„œ saveë•Œ ì˜í–¥ì„ ë°›ì€ ê²ƒ
- í•¨ìˆ˜().cache()ëŠ” cacheê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤.

---

**ì¶”í›„ì— ë„ì „í•´ë³¼ ê³¼ì œ(ë¯¸ì™„ì„±)**

- json normalize json flatten parquetë³€í™˜ (xmlì´ì•„ë‹Œ jsonìœ¼ë¡œì¨ ë°ì´í„° ë‹¤ë£¨ê¸°)
- json ë°‘ dictionaryí˜•íƒœë¡œ mongodbì €ì¥í•˜ê¸°
- lambdaì™€ cloudwatchë¡œ ec2 ìë™ìœ¼ë¡œ ë„ìš°ê³  ë‚´ë¦¬ê¸°
- boto3ë¡œec2ë„ìš°ê³  ë‚´ë¦¬ê¸°(with airflow?)
- airflow ë³€ìˆ˜ ì¸ì‹ argparse? ì•„ë‹ˆë©´ airflow variableì— ë³€ìˆ˜ ë“±ë¡ê°€ëŠ¥?

---

**ë” ì•Œì•„ë³´ê³  ì •ë¦¬í•´ì•¼ ë  ê²ƒ**

- xml vs lxml ì •í™•í•œ ì°¨ì´? í™œìš©ë„
- venvì˜ ì´ì ê³¼ í™˜ê²½ ë³€ìˆ˜ê°€ ì–´ë–»ê²Œ ì ìš©ë˜ëŠ” ì§€, airflowê°€ venvì•ˆì— ì„¤ì¹˜ëœë‹¤ë©´?
- boto3
- ì™œ saveDataAsCSV(getTodayCovidAffectedNumbers())ëŠ” 3ë²ˆì„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ”ì§€ ì°¾ì•„ë‚´ê¸°
- cache ì´í›„ì—” release??
- EMRì—ì„œ s3ì— íŒŒì¼ë„ ë§ˆì°¬ê°€ì§€ë¡œ access, secret, jar íŒŒì¼ 2ê°œ?
- airflowì—ì„œ ìì²´ì ìœ¼ë¡œ s3ì— ë¡œê·¸íŒŒì¼ ìƒì„±ì‹œí‚¤ëŠ” ë²•
